{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aws_tools import *\n",
    "\n",
    "# Update state machine and job def\n",
    "\n",
    "create_aws_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the s3 addresses for the dataset\n",
    "\n",
    "prefix = 'south_atlantic'\n",
    "resolution = '5min'\n",
    "staging_bucket = 'kerchunk-staging'\n",
    "run_name = 'south_atlantic-5min-2'\n",
    "\n",
    "files = get_dataset('nrel-pds-wtk', prefix=prefix, resolution=resolution)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the state machine input for this dataset\n",
    "\n",
    "s3_comb_ref_file = f'wtk/{prefix}/kerchunk_{resolution}_ref_s3.json'\n",
    "az_comb_ref_file = f'wtk/{prefix}/kerchunk_{resolution}_ref.json'\n",
    "create_state_machine_input(files, staging_bucket, s3_comb_ref_file, az_comb_ref_file, run_name=run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the state machine\n",
    "\n",
    "run_state_machine('kerchunk-h5', run_name=run_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the state machine fully executed without error, then there should now be a set of transformed h5 files, s3 refs and az refs, as well as a combined s3 ref file in the staging bucket. Use the test_staging.ipynb notebook to verify that the transformation was successful by loading the combined s3 ref file.\n",
    "\n",
    "Once you are satisfied, continue to the next cell to copy the data to Azure and generate the combined az ref file.\n",
    "\n",
    "Make sure to update the .env file with AWS credentials!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_s3_dataset_to_azure(files, staging_bucket, dry_run=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_ref_file = f'wtk/{prefix}/kerchunk_{resolution}_ref.json'\n",
    "create_combined_ref(files, staging_bucket, comb_ref_file=comb_ref_file, remote_protocol='abfs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once these tasks have finished, you can open the wtk example notebook and verify that the dataset can now be loaded from Azure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oedi-azure-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
